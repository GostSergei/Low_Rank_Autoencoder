{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Subset, Dataset, DataLoader\n",
    "\n",
    "import argparse\n",
    "\n",
    "\n",
    "from utils.script_utils import select_dataset, init_model, setup_dataset_training, save_checkpoint_from, upload_checkpoint_in\n",
    "from utils.loss import wasser_loss, entropy_limit_loss, kl_loss_from_ready\n",
    "from main_utils import get_models_class_list, get_base_model_parameters\n",
    "\n",
    "\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timer_start = timer() # start timer\n",
    "\n",
    "torch.manual_seed(0)\n",
    "# Defaults\n",
    "GOOD_DATASET_TYPE = ['MNIST', 'CIFAR10', 'CELEBA', 'FMNIST', 'FASHIONMNIST']\n",
    "GOOD_MODEL_TYPE = ['VAE', 'AE', 'LRAE', 'IRMAE']\n",
    "GOOD_ARCHITECTURE_TYPE = ['V1', 'NIPS']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Main work parameters\n",
    "DEVICE = 'cuda:2'\n",
    "DEVICE = 'cpu'\n",
    "MODEL_TYPE = 'AE'\n",
    "\n",
    "ARCHITECTURE_TYPE = 'NIPS'\n",
    "DATASET_TYPE = 'MNIST'\n",
    "\n",
    "ALPHA = 0\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "EPOCHS = 5\n",
    "# EPOCHS = 51\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "N_LATENT = 8\n",
    "\n",
    "# for LRAE\n",
    "N_BINS = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup runs\n"
     ]
    }
   ],
   "source": [
    "#### setup runs\n",
    "print(\"Setup runs\")\n",
    "if DATASET_TYPE.upper() in ['MNIST', 'FMNIST', 'FASHIONMNIST']:\n",
    "    # MODEL_NAME_PREF = f'test_bl_NIPS_{BATCH_SIZE}_{LEARNING_RATE}__'\n",
    "    MODEL_NAME_PREF = f'test_NIPS__'\n",
    "    SAVE_DIR = 'test_NIPS/data_n'\n",
    "    \n",
    "elif DATASET_TYPE.upper() in ['CIFAR10']:\n",
    "    MODEL_NAME_PREF = 'test_NIPS__'\n",
    "    SAVE_DIR = 'test_NIPS'\n",
    "    \n",
    "elif DATASET_TYPE.upper() in ['CELEBA']: \n",
    "    MODEL_NAME_PREF = f'test1_NIPS__'\n",
    "    SAVE_DIR = 'test_NIPS'\n",
    "else:\n",
    "   print(\"Warning! the default run setups was not setuped!\")\n",
    "   \n",
    "MODEL_NAME_PREF = 'Test_'\n",
    "SAVE_DIR = ''\n",
    "################### \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models were downloaded from 'models.NIPS_R1AE_MNIST'\n",
      "'torch.nn.functional.mse_loss'\t will be used as main_loss\n",
      "'None'\t will be used as additional_loss\n"
     ]
    }
   ],
   "source": [
    "# some const part from run_train \n",
    "models_class_list = get_models_class_list(DATASET_TYPE, ARCHITECTURE_TYPE) \n",
    "\n",
    "   \n",
    "### Model parameters\n",
    "\n",
    "# setup model parameters\n",
    "models_params = get_base_model_parameters(DATASET_TYPE, ARCHITECTURE_TYPE)\n",
    "#setup some parameters!!\n",
    "if N_LATENT != -1:\n",
    "    models_params['BOTTLENECK'] = N_LATENT\n",
    "###########\n",
    "\n",
    "BOTTLENECK =  models_params['BOTTLENECK']\n",
    "C_H_W = models_params['C_H_W']\n",
    "\n",
    "   \n",
    "# other Model parameters\n",
    "NONLINEARITY = nn.ReLU()\n",
    "models_params['NONLINEARITY'] = nn.ReLU()\n",
    "###\n",
    "\n",
    "\n",
    "# LRAE parameters\n",
    "# N_BINS, DROPOUT, TEMP, SAMPLING = 20, 0.0, 0.5, 'gumbell'\n",
    "DROPOUT, TEMP, SAMPLING = 0.0, 0.5, 'gumbell'\n",
    "\n",
    "models_params = models_params | {'N_BINS': N_BINS, 'DROPOUT':DROPOUT, 'SAMPLING':SAMPLING, 'TEMP': TEMP}\n",
    "##\n",
    "\n",
    "\n",
    "TRAIN_SIZE = -1\n",
    "TEST_SIZE = -1\n",
    "\n",
    "\n",
    "# EPOCH_SAVE = 50 # save and remain\n",
    "EPOCH_SAVE = 25 # save and remain\n",
    "\n",
    "\n",
    "EPOCH_SAVE_BACKUP = 5 # save and rewrite \n",
    "SHOW_LOSS_BACKUP = 5 # save and rewrite \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### setup main loss\n",
    "if MODEL_TYPE in ['VAE', 'LRAE']:\n",
    "    main_loss = torch.nn.functional.binary_cross_entropy\n",
    "    main_loss_str = 'torch.nn.functional.binary_cross_entropy'\n",
    "else: # AE\n",
    "    main_loss = torch.nn.functional.mse_loss\n",
    "    main_loss_str = 'torch.nn.functional.mse_loss'\n",
    "    \n",
    "print(f\"'{main_loss_str}'\\t will be used as main_loss\") \n",
    "##########################   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### setup additional loss\n",
    "if MODEL_TYPE.upper() in ['VAE']:\n",
    "    additional_loss, add_loss_str = kl_loss_from_ready, 'kl_loss_from_ready'\n",
    "elif  MODEL_TYPE.upper() in ['LRAE']:\n",
    "    # additional_loss, add_loss_str = wasser_loss, 'wasser_loss'\n",
    "    additional_loss, add_loss_str= entropy_limit_loss, 'entropy_limit_loss'\n",
    "else: # LRAE\n",
    "    additional_loss, add_loss_str = None, 'None' \n",
    "   \n",
    "\n",
    "print(f\"'{add_loss_str}'\\t will be used as additional_loss\")\n",
    "##########################\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input script data \n",
      "\n",
      "Main parameters:\n",
      "SAVE_DIR: \n",
      "DEVICE: cpu\n",
      "MODEL_TYPE: AE\n",
      "DATASET_TYPE: MNIST\n",
      "ARCHITECTURE_TYPE: NIPS\n",
      "BOTTLENECK: 8\n",
      "EPOCHS: 5\n",
      "\n",
      "\n",
      "\n",
      "All model parameters:\n",
      "IN_FEATURES: 1024\n",
      "BOTTLENECK: 8\n",
      "OUT_FEATURES: 8192\n",
      "DS_IN_CHANNELS: 1\n",
      "C_H_W: [128, 8, 8]\n",
      "MIDDLE_MATRIXES: 8\n",
      "NONLINEARITY: ReLU()\n",
      "N_BINS: 20\n",
      "DROPOUT: 0.0\n",
      "SAMPLING: gumbell\n",
      "TEMP: 0.5\n",
      "\n",
      "\n",
      "Training parameters:\n",
      "BATCH_SIZE: 256\n",
      "LEARNING_RATE: 0.0001\n",
      "ALPHA: 0\n",
      "EPOCHS: 5\n",
      "\n",
      "\n",
      "Dataset parameters:\n",
      "DATASET_TYPE: MNIST\n",
      "TRAIN_SIZE: -1\n",
      "TEST_SIZE: -1\n",
      "BATCH_SIZE: 256\n",
      "\n",
      "\n",
      "Other parameters\n",
      "NUM_WORKERS: 32\n",
      "EPOCH_SAVE: 25\n",
      "EPOCH_SAVE_BACKUP: 5\n",
      "SHOW_LOSS_BACKUP: 5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_params(param_list, param_names_list):\n",
    "    for param_name, param in zip(param_names_list, param_list):\n",
    "        print(f\"{param_name}: {param}\")\n",
    "    print()\n",
    "    \n",
    "\n",
    "# Show input data\n",
    "print('Input script data', '\\n')\n",
    "print('Main parameters:')\n",
    "in_param_list = [SAVE_DIR, DEVICE, MODEL_TYPE, DATASET_TYPE,  ARCHITECTURE_TYPE, BOTTLENECK, EPOCHS]\n",
    "in_param__names_list = ['SAVE_DIR', 'DEVICE', 'MODEL_TYPE', 'DATASET_TYPE', 'ARCHITECTURE_TYPE', 'BOTTLENECK', 'EPOCHS']\n",
    "print_params(in_param_list, in_param__names_list)\n",
    "print()\n",
    "print()\n",
    "\n",
    "\n",
    "print('All model parameters:')\n",
    "print_params(models_params.values(), models_params.keys())\n",
    "print()\n",
    "\n",
    "print('Training parameters:')\n",
    "in_param_list = [BATCH_SIZE, LEARNING_RATE, ALPHA,  EPOCHS]\n",
    "in_param__names_list = ['BATCH_SIZE', 'LEARNING_RATE', 'ALPHA', 'EPOCHS']\n",
    "print_params(in_param_list, in_param__names_list)\n",
    "print()\n",
    "\n",
    "print('Dataset parameters:')\n",
    "in_param_list = [DATASET_TYPE, TRAIN_SIZE, TEST_SIZE, BATCH_SIZE]\n",
    "in_param__names_list = ['DATASET_TYPE', 'TRAIN_SIZE', 'TEST_SIZE', 'BATCH_SIZE']\n",
    "print_params(in_param_list, in_param__names_list)\n",
    "print()\n",
    "\n",
    "\n",
    "## other parameters\n",
    "NUM_WORKERS = 32\n",
    "\n",
    "# Other parameters\n",
    "print('Other parameters')\n",
    "other_param_list = [NUM_WORKERS, EPOCH_SAVE, EPOCH_SAVE_BACKUP, SHOW_LOSS_BACKUP]\n",
    "other_param_names_list = ['NUM_WORKERS', 'EPOCH_SAVE', 'EPOCH_SAVE_BACKUP', 'SHOW_LOSS_BACKUP']\n",
    "for param_name, param in zip(other_param_names_list, other_param_list):\n",
    "    print(f\"{param_name}: {param}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking parameters\n",
    "assert MODEL_TYPE.upper() in GOOD_MODEL_TYPE, f\"Error, bad model type, select from: {GOOD_MODEL_TYPE}\"\n",
    "assert DATASET_TYPE.upper() in GOOD_DATASET_TYPE, f\"Error, bad dataset type, select from: {GOOD_DATASET_TYPE}\"\n",
    "assert ARCHITECTURE_TYPE.upper() in GOOD_ARCHITECTURE_TYPE, f\"Error, bad model architecture type, select from: {GOOD_ARCHITECTURE_TYPE}\"\n",
    "#############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Loading dataset: MNIST\n",
      "Dataset parameters:\n",
      "TRAIN_SIZE: 60000(60000)\n",
      "TEST_SIZE: 10000(10000)\n",
      "BATCH_SIZE = 256\n",
      "MNIST dataset logs:\n",
      "Img channel: 1\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "####### Dataset\n",
    "dataset_type = DATASET_TYPE\n",
    "print('\\n\\n')\n",
    "print(f'Loading dataset: {dataset_type}', flush=True)\n",
    "\n",
    "\n",
    "train_ds, test_ds, ds_train_size, df_test_size, ds_in_channels = select_dataset(DATASET_TYPE, GOOD_DATASET_TYPE)\n",
    "dl, dl_test = setup_dataset_training(train_ds, test_ds, BATCH_SIZE, num_workers=NUM_WORKERS)  \n",
    "models_params['DS_IN_CHANNELS'] = ds_in_channels\n",
    "\n",
    "\n",
    "TRAIN_SIZE = ds_train_size if TRAIN_SIZE == -1 else TRAIN_SIZE\n",
    "TEST_SIZE = df_test_size if TEST_SIZE == -1 else TEST_SIZE\n",
    "print(\"Dataset parameters:\")\n",
    "print(f\"TRAIN_SIZE: {TRAIN_SIZE}({ds_train_size})\")\n",
    "print(f\"TEST_SIZE: {TEST_SIZE}({df_test_size})\")\n",
    "for param, param_name in zip([BATCH_SIZE], [\"BATCH_SIZE\"] ):\n",
    "    print(f\"{param_name} = {param}\")\n",
    "\n",
    "print(f\"{DATASET_TYPE} dataset logs:\")\n",
    "print(\"Img channel:\", ds_in_channels)\n",
    "\n",
    "print('\\n\\n')\n",
    "###################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Initialization of the model\n",
      "model_name:  Test_MNIST__AE__8__0 \n",
      "\n",
      "\n",
      "ConvAE(\n",
      "  (nonlinearity): ReLU()\n",
      "  (down): Sequential(\n",
      "    (0): Dropout(p=0, inplace=False)\n",
      "    (1): DownsampleBlock(\n",
      "      (conv): Conv2d(1, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (nonlinearity): ReLU()\n",
      "    )\n",
      "    (2): DownsampleBlock(\n",
      "      (conv): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (nonlinearity): ReLU()\n",
      "    )\n",
      "    (3): DownsampleBlock(\n",
      "      (conv): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (nonlinearity): ReLU()\n",
      "    )\n",
      "    (4): DownsampleBlock(\n",
      "      (conv): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (nonlinearity): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (low_rank): InternalAutoencoder(\n",
      "    (low_rank_pants): PantsAE(\n",
      "      (out): Sequential(\n",
      "        (0): Linear(in_features=1024, out_features=8, bias=True)\n",
      "        (1): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (decoder): Sequential(\n",
      "      (0): Linear(in_features=8, out_features=8192, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (up): Sequential(\n",
      "    (0): UpsampleBlock(\n",
      "      (upsample): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (nonlinearity): ReLU()\n",
      "    )\n",
      "    (1): UpsampleBlock(\n",
      "      (upsample): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (nonlinearity): ReLU()\n",
      "    )\n",
      "    (2): UpsampleBlock(\n",
      "      (upsample): ConvTranspose2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (nonlinearity): Tanh()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "AE was initialized\n",
      "Save PATH: Test_MNIST__AE__8__0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "###################### Initialization of the model\n",
    "\n",
    "# DEVICE, \n",
    "\n",
    "device = DEVICE\n",
    "model_name = MODEL_NAME_PREF + f\"{DATASET_TYPE}__{MODEL_TYPE}__{BOTTLENECK}__{ALPHA}\"\n",
    "\n",
    "print('\\n\\n')\n",
    "print(\"Initialization of the model\")\n",
    "print(\"model_name: \", model_name, '\\n\\n' )\n",
    "\n",
    "model = init_model(MODEL_TYPE, GOOD_MODEL_TYPE,  models_class_list, models_params, device)\n",
    "\n",
    "print(model)\n",
    "\n",
    "print(f\"{MODEL_TYPE} was initialized\")\n",
    "PATH = os.path.join(SAVE_DIR, model_name)\n",
    "print('Save PATH:', PATH, flush=True)\n",
    "#################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training of the model\n"
     ]
    }
   ],
   "source": [
    "####### Training\n",
    "print(\"Training of the model\", flush=True)\n",
    "device = DEVICE\n",
    "\n",
    "\n",
    "# setup training\n",
    "# criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "\n",
    "PATH = PATH\n",
    "EPOCHS = EPOCHS\n",
    "\n",
    "\n",
    "loss_list_train = []\n",
    "loss_train_cum = 0\n",
    "\n",
    "loss_list_test = []\n",
    "loss_test_cum = 0\n",
    "i = 0\n",
    "loss = 0\n",
    "epoch_0 = 0 \n",
    "\n",
    "#time\n",
    "epoch_time_list = []\n",
    "epoch_t1 = None\n",
    "\n",
    "alpha = ALPHA\n",
    "\n",
    "epoch_save_backup = EPOCH_SAVE_BACKUP\n",
    "epoch_save = EPOCH_SAVE\n",
    "show_loss_backup = SHOW_LOSS_BACKUP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training will be continue!!!\n",
      "Loaded epoch: 9\n",
      "Loaded final loss: tensor(0.0498, device='cuda:2', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# load_path = None\n",
    "load_path = 'Test_MNIST__AE__8__0__9__end.pth'\n",
    "# loading chekpoint \n",
    "if load_path is not None:\n",
    "    print('The training will be continue!!!')\n",
    "    checkpoint = upload_checkpoint_in(load_path, model=model, optimizer=optimizer, device=device)\n",
    "    print(\"Loaded epoch:\", checkpoint['epoch'])\n",
    "    print(\"Loaded final loss:\", checkpoint['loss'])\n",
    "\n",
    "    loss_list_train = checkpoint['loss_list_train']\n",
    "    loss_list_test = checkpoint['loss_list_test']\n",
    "    epoch_time_list = checkpoint['epoch_time_list'] if 'epoch_time_list' in checkpoint.keys() else None\n",
    "    \n",
    "    epoch_0 = checkpoint['epoch'] +1\n",
    "    \n",
    "    del checkpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### additional \n",
    "timer_start = timer()\n",
    "# epoch_0 = epoch + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [11:38<17:28, 349.44s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/s.gostilovich/Low_rank AE/Low_Rank_Autoencoder/Autoencoder training.ipynb Cell 18\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfedor/home/s.gostilovich/Low_rank%20AE/Low_Rank_Autoencoder/Autoencoder%20training.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39m# Zero gradients, perform a backward pass, and update the weights.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfedor/home/s.gostilovich/Low_rank%20AE/Low_Rank_Autoencoder/Autoencoder%20training.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=52'>53</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bfedor/home/s.gostilovich/Low_rank%20AE/Low_Rank_Autoencoder/Autoencoder%20training.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfedor/home/s.gostilovich/Low_rank%20AE/Low_Rank_Autoencoder/Autoencoder%20training.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfedor/home/s.gostilovich/Low_rank%20AE/Low_Rank_Autoencoder/Autoencoder%20training.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=55'>56</a>\u001b[0m \u001b[39m# accumulate loss\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/lib64/python3.9/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    490\u001b[0m )\n",
      "File \u001b[0;32m~/.venv/lib64/python3.9/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Training\n",
    "model.train()\n",
    "optimizer.zero_grad()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "    epoch = epoch_0 + epoch\n",
    "    # time\n",
    "    epoch_t2 = timer()\n",
    "    if epoch_t1 is not None:\n",
    "        epoch_time_list += [epoch_t2 - epoch_t1]\n",
    "    epoch_t1 = epoch_t2\n",
    "    \n",
    "    \n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "        \n",
    "    # Training\n",
    "    model.train() # Model to train\n",
    "    epoch_t1 = timer()\n",
    "    for x_batch, y_batch in dl:\n",
    "        \n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        # model forward\n",
    "        # 2d downsampling\n",
    "        x_down = model.down(x_batch)\n",
    "        B, C, H, W = x_down.shape\n",
    "        x_flat = x_down.view(B,C*H*W)\n",
    "        \n",
    "        encoded_out_dim, factors_probability = model.low_rank.low_rank_pants(x_flat)\n",
    "        decoded_1d = model.low_rank.decoder(encoded_out_dim)\n",
    "        \n",
    "        # print(B, C, H, W )\n",
    "        \n",
    "        # 2d upsampling\n",
    "        C, H, W = C_H_W\n",
    "        decoded_2d_small = decoded_1d.view(B, C, H, W)\n",
    "        decoded_2d = model.up(decoded_2d_small)\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        # loss\n",
    "        loss = main_loss(decoded_2d, x_batch)\n",
    "        # loss = main_loss(decoded_2d.view(-1), x_batch.view(-1))\n",
    "        if additional_loss is not None:\n",
    "            loss += alpha*additional_loss(factors_probability)\n",
    "        \n",
    "\n",
    "            \n",
    "        # Zero gradients, perform a backward pass, and update the weights.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # accumulate loss\n",
    "        loss_train_cum += loss.item()\n",
    "        \n",
    "        # validation and saving\n",
    "        i += 1\n",
    "        if i % 100 == 0:\n",
    "            loss_list_train.append(loss_train_cum/100)\n",
    "            loss_train_cum = 0\n",
    "            with torch.no_grad():\n",
    "                model.eval() # put to eval\n",
    "                \n",
    "                for x_batch, y_batch in dl_test:\n",
    "                    # model forward\n",
    "                    x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "                    x_decoded = model(x_batch)\n",
    "\n",
    "                    loss_test = main_loss(x_decoded, x_batch)\n",
    "                    loss_test_cum += loss_test.item()\n",
    "                    \n",
    "            assert torch.isnan(x_decoded).sum() == 0, f\"Error! Nan values ({torch.isnan(x_decoded).sum()}) in models output\"\n",
    "      \n",
    "            # save to list\n",
    "            loss_list_test.append(loss_test_cum/len(dl_test))\n",
    "            loss_test_cum = 0\n",
    "          \n",
    "    # backup saving  \n",
    "    if epoch % epoch_save == 0:\n",
    "        \n",
    "        save_checkpoint_from(PATH + f\"__{epoch}.pth\", model, optimizer,  epoch=epoch, loss=loss.item(), \n",
    "                    loss_list_train=loss_list_train, loss_list_test=loss_list_test,\n",
    "                    epoch_time_list=epoch_time_list)\n",
    "        \n",
    "\n",
    "        epoch_previous = epoch\n",
    "            \n",
    "    # backup saving  \n",
    "    if epoch%epoch_save_backup == 0:\n",
    "        \n",
    "        save_checkpoint_from(PATH + f\"__backup.pth\", model, optimizer,  epoch=epoch, loss=loss.item(), \n",
    "                    loss_list_train=loss_list_train, loss_list_test=loss_list_test,\n",
    "                    epoch_time_list=epoch_time_list)\n",
    "        \n",
    "        \n",
    "  \n",
    "        epoch_previous = epoch\n",
    "      \n",
    "    # loss printing        \n",
    "    if (epoch % show_loss_backup == (show_loss_backup-1)) or (epoch == EPOCHS -1):\n",
    "        fig = plt.figure(figsize=(6,3))\n",
    "        plt.plot(loss_list_train, alpha=0.5, label='train')\n",
    "        plt.plot(loss_list_test, alpha=0.5, label='test')\n",
    "        plt.legend()\n",
    "        plt.savefig( PATH  + \"_loss.jpg\")\n",
    "        plt.close()\n",
    "        pass\n",
    "            \n",
    "\n",
    "print(\"Finishing of the training...\")\n",
    "\n",
    "save_checkpoint_from(PATH + f\"__{epoch}__end.pth\", model, optimizer,  epoch=epoch, loss=loss.item(), \n",
    "                    loss_list_train=loss_list_train, loss_list_test=loss_list_test,\n",
    "                    epoch_time_list=epoch_time_list)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#######################\n",
    "\n",
    "\n",
    "timer_end = timer()\n",
    "print(f\"Elapsed time: {timer_end - timer_start:.2f} second\") # Time in seconds, e.g. 5.38091952400282\n",
    "print(\"Mean full epoch time:\", f\"{np.asarray(epoch_time_list).mean().item():.1f}\")\n",
    "print(f\"Model training for {model_name} was successfully finished and saved!\")\n",
    "print('\\n\\n\\n\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
