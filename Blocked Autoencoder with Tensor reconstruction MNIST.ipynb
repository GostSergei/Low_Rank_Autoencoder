{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07666d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1de55837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = 'cuda:0'\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adae70d",
   "metadata": {},
   "source": [
    "# Low Rank Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb51761",
   "metadata": {},
   "source": [
    "### Blocked autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "420f68e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # SHIFTED MASKING FUNCTION - didn't prove to be of much help\n",
    "# def masking(normalized_factors, mask_size):\n",
    "#     B, out_features, n_bins = normalized_factors.shape\n",
    "#     assert n_bins % mask_size == 0 , f'n_bins={n_bins} must be divisible by mask_size={mask_size}'\n",
    "#     eps = 1e-9\n",
    "#     mask = torch.ones(B, out_features, n_bins).to(device)\n",
    "#     mask[::,::,0:mask_size] = 0\n",
    "#     mask = torch.roll(mask, torch.randint(low=0,high=n_bins-mask_size+1, size=(1,)).item(), dims=-1)\n",
    "#     dropped_factors = mask*normalized_factors + eps\n",
    "#     return dropped_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49f6f0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.pants import LowRankAutoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c686d8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 20])\n",
      "tensor(0)\n"
     ]
    }
   ],
   "source": [
    "LRA = LowRankAutoencoder(in_features=20, out_features=2, n_bins=20, grid=torch.arange(1,21)/20,\n",
    "                         dropout=0.0, nonlinearity=nn.ELU(),\n",
    "                         sampling='vector', temperature=0.1).to(device)\n",
    "\n",
    "ttt = torch.rand(16, 20).to(device)\n",
    "print(LRA(ttt).shape)\n",
    "print(torch.sum(torch.isnan(LRA(ttt))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90850ff3",
   "metadata": {},
   "source": [
    "# Convolutional autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31f06b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.R1AE import ConvLRAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c96da9a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 32, 32])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdl = ConvLRAE(in_features=256*2*2, out_features=2, n_bins=20, grid=torch.arange(1,21)/20,\n",
    "                sampling='vector', dropout=0.1, nonlinearity=nn.ReLU()\n",
    "              ).to(device)\n",
    "\n",
    "ttt = torch.randn(10, 1, 32, 32).to(device)\n",
    "\n",
    "mdl(ttt).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba9eecc",
   "metadata": {},
   "source": [
    "### Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f387b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MNIST_DS(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float)\n",
    "        self.X = self.X/torch.max(self.X) # normalizing to 1.\n",
    "        self.Y = torch.tensor(Y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.Y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.X[idx], self.Y[idx]\n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb87ac7",
   "metadata": {},
   "source": [
    "### Torchvision dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c14de7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Subset, Dataset, DataLoader\n",
    "\n",
    "train_ds_mnist = torchvision.datasets.MNIST('./files/', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                                 transforms.Resize(32),\n",
    "                                 torchvision.transforms.ToTensor(),\n",
    "                             ]))\n",
    "test_ds_mnist = torchvision.datasets.MNIST('./files/', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                                 transforms.Resize(32),\n",
    "                                 torchvision.transforms.ToTensor(),\n",
    "                             ]))\n",
    "\n",
    "# dataset and dataloader\n",
    "TRAIN_SIZE = 10000\n",
    "TEST_SIZE = 5000\n",
    "BATCH_SIZE = 32\n",
    "dl = DataLoader(train_ds_mnist, batch_size=BATCH_SIZE,     num_workers=1)\n",
    "dl_test = DataLoader(test_ds_mnist, batch_size=BATCH_SIZE, num_workers=1)\n",
    "\n",
    "#full dataset train\n",
    "FULL_TRAIN_SIZE = 10000\n",
    "dl_full = DataLoader(train_ds_mnist, batch_size=FULL_TRAIN_SIZE)\n",
    "for x, y in dl_full:\n",
    "    X_full_train = x\n",
    "    targets = y\n",
    "    break\n",
    "\n",
    "#full dataset train\n",
    "FULL_TEST_SIZE = 10000\n",
    "dl_full = DataLoader(test_ds_mnist, batch_size=FULL_TEST_SIZE)\n",
    "for x, y in dl_full:\n",
    "    X_full_test = x\n",
    "    targets_test = y\n",
    "    break\n",
    "\n",
    "print(X_full_train.shape)\n",
    "print(torch.max(X_full_train))\n",
    "print(targets.unique(return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae77800",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b438f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "IN_FEATURES = 256*2*2\n",
    "OUT_FEATURES = 8\n",
    "N_BINS = 20\n",
    "GRID = torch.arange(1,N_BINS+1).to(device)/N_BINS\n",
    "\n",
    "DROPOUT = 0.0\n",
    "TEMP = 0.5\n",
    "SAMPLING = 'gumbell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2746afae",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "model = ConvLRAE(\n",
    "                IN_FEATURES, OUT_FEATURES, N_BINS, GRID,\n",
    "                dropout=DROPOUT, nonlinearity=nn.ELU(),\n",
    "                sampling=SAMPLING, temperature=TEMP,\n",
    "                ).to(device)\n",
    "\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960af25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "model.train()\n",
    "optimizer.zero_grad()\n",
    "\n",
    "loss_list_train = []\n",
    "loss_train_cum = 0\n",
    "\n",
    "loss_list_test = []\n",
    "loss_test_cum = 0\n",
    "i = 0\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    \n",
    "    for x_batch, y_batch in dl:\n",
    "\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "        # model forward\n",
    "        # 2d downsampling\n",
    "        x_down = model.down(x_batch)\n",
    "        B, C, H, W = x_down.shape\n",
    "        x_flat = x_down.view(B,C*H*W)\n",
    "        encoded_out_dim, factors_probability = model.low_rank.low_rank_pants(x_flat)\n",
    "        decoded_1d = model.low_rank.decoder(encoded_out_dim)\n",
    "        # 2d upsampling\n",
    "        decoded_2d_small = decoded_1d.view(B, C, H, W)\n",
    "        decoded_2d = model.up(decoded_2d_small)\n",
    "\n",
    "#         loss_entropy = torch.sum(torch.log(factors_probability+1e-9)*factors_probability,dim=-1)\n",
    "        factors_probability = nn.Softmax(dim=-1)(factors_probability)\n",
    "        loss_entropy = torch.sum(torch.log(factors_probability+1e-9)*factors_probability,dim=-1)\n",
    "        loss = criterion(decoded_2d.view(-1), x_batch.view(-1)) + 1e-2*torch.mean(torch.exp(loss_entropy))\n",
    "        # Zero gradients, perform a backward pass, and update the weights.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # accumulate loss\n",
    "        loss_train_cum += loss.item()\n",
    "        \n",
    "        # validation and saving\n",
    "        i += 1\n",
    "        if i % 100 == 0:\n",
    "            loss_list_train.append(loss_train_cum/100)\n",
    "            loss_train_cum = 0\n",
    "            with torch.no_grad():\n",
    "                model.eval() # put to eval\n",
    "                for x_batch, y_batch in dl_test:\n",
    "                    # model forward\n",
    "                    x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "                    x_decoded = model(x_batch)\n",
    "\n",
    "                    loss_test = criterion(x_decoded.view(-1), x_batch.view(-1))\n",
    "                    loss_test_cum += loss_test.item()\n",
    "                model.train() # put back to train\n",
    "            # save to list\n",
    "            loss_list_test.append(loss_test_cum/len(dl_test))\n",
    "            loss_test_cum = 0\n",
    "            \n",
    "    #     break\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283abf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.sum(torch.isnan(x_decoded)))\n",
    "print(torch.mean(torch.exp(loss_entropy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87827f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH = 'R1AE_MNIST_out=8_nbins=20_entropy=1e-2'\n",
    "# EPOCH_NUM = 100\n",
    "# torch.save({\n",
    "#     'epoch': EPOCH_NUM,\n",
    "#     'model_state_dict': model.state_dict(),\n",
    "#     'optimizer_state_dict': optimizer.state_dict(),\n",
    "#     'loss': loss,\n",
    "#     'loss_list_train': loss_list_train,\n",
    "#     'loss_list_test': loss_list_test,\n",
    "\n",
    "#     }, PATH + f\"__{epoch}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cada05c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "35a639ca",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74d796f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,3))\n",
    "plt.plot(loss_list_train, alpha=0.5, label='train')\n",
    "plt.plot(loss_list_test, alpha=0.5, label='test')\n",
    "# plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.savefig('images/R1AE_MNIST_Gumbell_torch.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9a00ce",
   "metadata": {},
   "source": [
    "### Reconstruction and distribution graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32fde44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inf_by_layers(model, x_batch):\n",
    "    # forward pass with intermediate layers\n",
    "    x_down = model.down(x_batch)\n",
    "    B, C, H, W = x_down.shape\n",
    "    x_flat = x_down.view(B,C*H*W)\n",
    "    encoded_out_dim, factors_probability = model.low_rank.low_rank_pants(x_flat)\n",
    "    decoded_1d = model.low_rank.decoder(encoded_out_dim)\n",
    "    # 2d upsampling\n",
    "    decoded_2d_small = decoded_1d.view(B, C, H, W)\n",
    "    decoded_2d = model.up(decoded_2d_small)\n",
    "\n",
    "    return decoded_2d, encoded_out_dim, factors_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d319e66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3,10, figsize=(11,4))\n",
    "\n",
    "for i in range(0,10):\n",
    "    INDX=i+80\n",
    "    #true\n",
    "    \n",
    "    #pred\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        x_batch = X_full_test[INDX].unsqueeze(0).to(device)\n",
    "        \n",
    "        # plotting original images\n",
    "        axs[0,i].imshow(x_batch[0,0].cpu().detach().numpy()) # mnist\n",
    "\n",
    "        # forward pass with intermediate layers\n",
    "        decoded_2d, encoded_out_dim, factors_probability = inf_by_layers(model, x_batch)\n",
    "        \n",
    "    axs[1,i].imshow(decoded_2d.cpu().detach().numpy()[0,0]) # mnist\n",
    "\n",
    "    \n",
    "    # 1d probabilities\n",
    "    for j in range(factors_probability.shape[1]):\n",
    "        axs[2,i].plot(nn.Softmax(dim=-1)(factors_probability)[0,j,::].cpu().detach().numpy())\n",
    "#         axs[2,i].set_ylim(-0.1,1.1)\n",
    "        \n",
    "    axs[0,i].set_xticks([])\n",
    "    axs[0,i].set_yticks([])\n",
    "    axs[1,i].set_xticks([])\n",
    "    axs[1,i].set_yticks([])\n",
    "    axs[2,i].set_xticks([])\n",
    "    axs[2,i].set_yticks([])\n",
    "\n",
    "# plt.savefig('MNIST_softmax_plus_Reconstruction.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63d592f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward whole dataset\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    # mnist\n",
    "    decoded_2d, encoded_out_dim, factors_probability = inf_by_layers(model, X_full_test.to(device))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c49bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "fig, axs = plt.subplots(1,10, figsize=(12,2))\n",
    "fbs = nn.Softmax(dim=-1)(factors_probability)\n",
    "for i in range(0,10):\n",
    "    f1, f2 = fbs[::,0,::], fbs[::,1,::]\n",
    "    probs = np.zeros((f1.shape[-1],f1.shape[-1]))\n",
    "    # selecting the encoded distibution vectors\n",
    "    f1_cls, f2_cls = f1[targets_test == i], f2[targets_test == i]\n",
    "    for f1_vec, f2_vec in zip(f1_cls, f2_cls):\n",
    "        probs += np.outer(f1_vec.cpu().detach().numpy(), f2_vec.cpu().detach().numpy())\n",
    "            \n",
    "    axs[i].imshow(probs)\n",
    "#     axs[i].set_xticks(model.range.detach().numpy()/20)\n",
    "#     axs[i].set_yticks(model.range.detach().numpy()/20)\n",
    "    \n",
    "    axs[i].set_title(f'{i}')\n",
    "\n",
    "plt.savefig('MNIST_prob_2d_distribution.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f87224",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3dae68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward whole dataset\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    # mnist\n",
    "    decoded_2d_train, encoded_out_dim_train, factors_probability_train = inf_by_layers(model, X_full_train.to(device))\n",
    "    decoded_2d_test, encoded_out_dim_test, factors_probability_test = inf_by_layers(model, X_full_test.to(device))  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51f0755",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc38db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "enc_pca_train = pca.fit_transform(encoded_out_dim_train.cpu().detach().numpy())\n",
    "enc_pca_test = pca.fit_transform(encoded_out_dim_test.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e07c327",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2, figsize=(10,5), sharey=True)\n",
    "for cls in [0,1,2,3,4,5,6,7,8,9]:\n",
    "    X_pca_train = enc_pca_train[targets == cls]\n",
    "    axs[0].scatter(X_pca_train[::,0], X_pca_train[::,1], label=cls, alpha=0.4)\n",
    "    \n",
    "    X_pca_test = enc_pca_test[targets_test == cls]\n",
    "    axs[1].scatter(X_pca_test[::,0], X_pca_test[::,1], label=cls, alpha=0.4)\n",
    "\n",
    "axs[0].set_title('PCA Train')\n",
    "axs[1].set_title('PCA Test')\n",
    "plt.legend(bbox_to_anchor=(1.2, 1))\n",
    "plt.savefig('MNIST_Gumbell_PCA.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cceface",
   "metadata": {},
   "source": [
    "### T-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033c28d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2)\n",
    "\n",
    "enc_tsne_train = tsne.fit_transform(encoded_out_dim_train.cpu().detach().numpy())\n",
    "enc_tsne_test = tsne.fit_transform(encoded_out_dim_test.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c79f3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2, figsize=(10,5), sharey=True)\n",
    "for cls in [0,1,2,3,4,5,6,7,8,9]:\n",
    "    X_tsne_train = enc_tsne_train[targets == cls]\n",
    "    axs[0].scatter(X_tsne_train[::,0], X_tsne_train[::,1], label=cls, alpha=0.4)\n",
    "    \n",
    "    X_tsne_test = enc_tsne_test[targets_test == cls]\n",
    "    axs[1].scatter(X_tsne_test[::,0], X_tsne_test[::,1], label=cls, alpha=0.4)\n",
    "\n",
    "axs[0].set_title('TSNE Train')\n",
    "axs[1].set_title('TSNE Test')\n",
    "plt.legend(bbox_to_anchor=(1.2, 1))\n",
    "plt.savefig('MNIST_Gumbell_TSNE.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3209a486",
   "metadata": {},
   "source": [
    "# Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c0fb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward whole dataset\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    # mnist\n",
    "    decoded_2d_train, encoded_out_dim_train, factors_probability_train = inf_by_layers(model, X_full_train.to(device))\n",
    "    decoded_2d_test, encoded_out_dim_test, factors_probability_test = inf_by_layers(model, X_full_test.to(device))  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722be798",
   "metadata": {},
   "source": [
    "### Multivariate Gaussian Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b5b449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "EMB_SIZE = 8\n",
    "\n",
    "mean_vec = torch.mean(encoded_out_dim_train, dim=0)\n",
    "std_vec = torch.std(encoded_out_dim_train, dim=0)\n",
    "\n",
    "fig, axs = plt.subplots(3,10, figsize=(12,3))\n",
    "for i in range(0,10):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        for j in range(3):\n",
    "            rand = torch.randn(EMB_SIZE).to(device)*std_vec + mean_vec\n",
    "            x_down = model.down(x_batch)\n",
    "            B, C, H, W = (1,128,4,4)\n",
    "            decoded_inter_dim =         model.low_rank.intermediate_decoder(rand.to(device))\n",
    "            decoded_1d =                model.low_rank.decoder(decoded_inter_dim)\n",
    "            decoded_2d_small = decoded_1d.view(B, C, H, W)\n",
    "            decoded_2d = model.up(decoded_2d_small)\n",
    "\n",
    "            axs[j,i].imshow(decoded_2d.cpu().detach().numpy()[0,0])\n",
    "\n",
    "\n",
    "\n",
    "    axs[0,i].set_xticks([])\n",
    "    axs[0,i].set_yticks([])\n",
    "    axs[1,i].set_xticks([])\n",
    "    axs[1,i].set_yticks([])\n",
    "    axs[2,i].set_xticks([])\n",
    "    axs[2,i].set_yticks([])\n",
    "#     axs[i].set_title(f'{i}')\n",
    "\n",
    "plt.savefig('MNIST_Single_Gaussian_Generation.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6316a6e8",
   "metadata": {},
   "source": [
    "### K-means Gaussian Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24b0784",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_from_gaussians(mean_std_list, N_CLUSTERS, fixed_idx=None):\n",
    "    if not fixed_idx:\n",
    "        idx = torch.randint(low=0, high=N_CLUSTERS, size=(1,))\n",
    "    else:\n",
    "        idx = fixed_idx\n",
    "    mean, std = mean_std_list[idx]\n",
    "    rand = torch.randn(mean.shape)*std + mean\n",
    "    return rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff94bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_points_train = encoded_out_dim_train.cpu().detach().numpy()\n",
    "enc_points_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd08961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating best k\n",
    "# from sklearn.cluster import KMeans\n",
    "# cost =[]\n",
    "# k_nums = [5,10,15,20,30,50,100]\n",
    "# for i in k_nums:\n",
    "#     KM = KMeans(n_clusters = i, max_iter = 500, n_init=5)\n",
    "#     KM.fit(enc_points_train)\n",
    "#     cost.append(KM.inertia_) \n",
    "# plt.figure(figsize=(4,2))\n",
    "# plt.plot(k_nums, cost)\n",
    "# plt.grid()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2b09bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "N_CLUSTERS = 30\n",
    "\n",
    "kmeans = KMeans(n_clusters=N_CLUSTERS, n_init=100)\n",
    "kmeans.fit(enc_points_train)\n",
    "\n",
    "# filling list of gaussian means and stds\n",
    "mean_std_list = []\n",
    "for clstr_idx in np.unique(kmeans.labels_):\n",
    "    mean = kmeans.cluster_centers_[clstr_idx]\n",
    "    std = np.std(enc_points_train[kmeans.labels_== clstr_idx], axis=0)\n",
    "    mean_std_list.append([torch.from_numpy(mean), torch.from_numpy(std)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fad34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_from_gaussians(mean_std_list, N_CLUSTERS, fixed_idx=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce9a1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "EMB_SIZE = 8\n",
    "\n",
    "mean_vec = torch.mean(encoded_out_dim_train, dim=0)\n",
    "std_vec = torch.std(encoded_out_dim_train, dim=0)\n",
    "\n",
    "fig, axs = plt.subplots(N_CLUSTERS,10, figsize=(12,N_CLUSTERS))\n",
    "for i in range(0,10):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        for j in range(N_CLUSTERS):\n",
    "            rand = generate_from_gaussians(mean_std_list, N_CLUSTERS, fixed_idx=j)\n",
    "            x_down = model.down(x_batch)\n",
    "            B, C, H, W = (1,128,4,4)\n",
    "            decoded_inter_dim =         model.low_rank.intermediate_decoder(rand.to(device))\n",
    "            decoded_1d =                model.low_rank.decoder(decoded_inter_dim)\n",
    "            decoded_2d_small = decoded_1d.view(B, C, H, W)\n",
    "            decoded_2d = model.up(decoded_2d_small)\n",
    "\n",
    "            axs[j,i].imshow(decoded_2d.cpu().detach().numpy()[0,0])\n",
    "\n",
    "            axs[j,i].set_xticks([])\n",
    "            axs[j,i].set_yticks([])\n",
    "\n",
    "plt.savefig('MNIST_Multtiple_Gaussian_Generation.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77175b4d",
   "metadata": {},
   "source": [
    "### K-means Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938ebd72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7005a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_from_clusters(clusters_tensor, grid, fixed_idx = None):\n",
    "    if not fixed_idx:\n",
    "        idx = torch.randint(low=0, high=N_CLUSTERS, size=(1,))\n",
    "    else:\n",
    "        idx = fixed_idx\n",
    "    n_clust, enc_dim, n_bins = clusters_tensor.shape\n",
    "    clusters_logits = torch.log(clusters_tensor + 1e-9)\n",
    "    enc = gumbell_torch_sampling(clusters_logits, grid, temperature=0.5)\n",
    "    return enc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9272b8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# probability flat matrix\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "probs_train = nn.Softmax(dim=-1)(factors_probability_train).cpu().detach().numpy()\n",
    "probs_train = probs_train \n",
    "probs_train_smooth = gaussian_filter1d(probs_train, sigma=1.) # smoothing\n",
    "\n",
    "prob_flat_train = probs_train.reshape(probs_train.shape[0], -1)\n",
    "prob_flat_smooth_train = probs_train_smooth.reshape(probs_train_smooth.shape[0], -1)\n",
    "\n",
    "\n",
    "plt.imshow(prob_flat_smooth_train[::200,::])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd59a06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(np.isnan(prob_flat_train)))\n",
    "print(np.sum(np.isinf(prob_flat_train)))\n",
    "print(np.sum(prob_flat_train < 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6fab41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating best k\n",
    "# from sklearn.cluster import KMeans\n",
    "# cost =[]\n",
    "# k_nums = [5,10,15,20,30,50,100,500,1000]\n",
    "# for i in k_nums:\n",
    "#     KM = KMeans(n_clusters = i, max_iter = 500, n_init=5)\n",
    "#     KM.fit(prob_flat_train)\n",
    "#     cost.append(KM.inertia_) \n",
    "# plt.figure(figsize=(4,2))\n",
    "# plt.plot(k_nums, cost)\n",
    "# plt.grid()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd545444",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "N_CLUSTERS = 30\n",
    "\n",
    "kmeans = KMeans(n_clusters=N_CLUSTERS, n_init=100)\n",
    "\n",
    "kmeans.fit(prob_flat_smooth_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45cd6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    ".shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee14e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_centroids = []\n",
    "for clstr_idx in np.unique(kmeans.labels_):\n",
    "    if clstr_idx == -1:\n",
    "        continue\n",
    "    centroid = np.mean(prob_flat_train[kmeans.labels_==clstr_idx], axis=0)\n",
    "    kmeans_centroids.append(centroid)\n",
    "np.array(kmeans_centroids).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a924acd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_tensor = torch.from_numpy(np.array(kmeans_centroids)).view(N_CLUSTERS,8,20)\n",
    "\n",
    "print(torch.sum(torch.isnan(clusters_tensor)))\n",
    "print(torch.sum(torch.isinf(clusters_tensor)))\n",
    "print(torch.sum(clusters_tensor < 0))\n",
    "clusters_tensor[clusters_tensor<0] = 0.\n",
    "print(torch.sum(clusters_tensor < 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315af479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "EMB_SIZE = 8\n",
    "\n",
    "fig, axs = plt.subplots(N_CLUSTERS,10, figsize=(12,N_CLUSTERS))\n",
    "for i in range(0,10):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        for j in range(N_CLUSTERS):\n",
    "            rand = generate_from_clusters(clusters_tensor, grid=torch.arange(1,21)/20, fixed_idx=j)\n",
    "            x_down = model.down(x_batch)\n",
    "            B, C, H, W = (1,128,4,4)\n",
    "            decoded_inter_dim =         model.low_rank.intermediate_decoder(rand.to(device))\n",
    "            decoded_1d =                model.low_rank.decoder(decoded_inter_dim)\n",
    "            decoded_2d_small = decoded_1d.view(B, C, H, W)\n",
    "            decoded_2d = model.up(decoded_2d_small)\n",
    "\n",
    "            axs[j,i].imshow(decoded_2d.cpu().detach().numpy()[0,0])\n",
    "\n",
    "            axs[j,i].set_xticks([])\n",
    "            axs[j,i].set_yticks([])\n",
    "\n",
    "plt.savefig('MNIST_K_means_Generation.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5c4b17",
   "metadata": {},
   "source": [
    "### Numbers transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5936094e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "EMB_SIZE = 8\n",
    "\n",
    "fig, axs = plt.subplots(4,20, figsize=(14,4))\n",
    "for i in range(0,20):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "            \n",
    "        for j in range(4):\n",
    "            # encode two numbers\n",
    "            # NUMBER 1\n",
    "            IDX_1 = j+2\n",
    "            x_down1 = model.down(X_full_test[IDX_1].unsqueeze(0).to(device))\n",
    "            B, C, H, W = x_down1.shape\n",
    "            x_flat1 = x_down1.view(B,C*H*W)\n",
    "            encoded_inter_dim1 =                    model.low_rank.encoder(x_flat1)\n",
    "            encoded_out_dim1, factors_probability1 = model.low_rank.low_rank_pants(encoded_inter_dim1)\n",
    "\n",
    "            # NUMBER 2\n",
    "            IDX_2 = j+20\n",
    "            x_down2 = model.down(X_full_test[IDX_2].unsqueeze(0).to(device))\n",
    "            B, C, H, W = x_down2.shape\n",
    "            x_flat2 = x_down2.view(B,C*H*W)\n",
    "            encoded_inter_dim2 =                    model.low_rank.encoder(x_flat2)\n",
    "            encoded_out_dim2, factors_probability2 = model.low_rank.low_rank_pants(encoded_inter_dim2)\n",
    "\n",
    "            # decode and plot\n",
    "            new_encoded = (1-i/axs.shape[-1])*encoded_out_dim1 + (i/axs.shape[-1])*encoded_out_dim2\n",
    "            decoded_inter_dim =  model.low_rank.intermediate_decoder(new_encoded)\n",
    "            decoded_1d = model.low_rank.decoder(decoded_inter_dim)\n",
    "            decoded_2d_small = decoded_1d.view(B, C, H, W)\n",
    "            decoded_2d = model.up(decoded_2d_small)\n",
    "\n",
    "            axs[j,i].imshow(decoded_2d.cpu().detach().numpy()[0,0])\n",
    "            axs[j,i].set_xticks([])\n",
    "            axs[j,i].set_yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06848709",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
